{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Continuous_Control Project Report - DRLND."]},{"cell_type":"markdown","metadata":{},"source":["[//]: # (Image References)\n","[image1]: https://user-images.githubusercontent.com/10624937/43851024-320ba930-9aff-11e8-8493-ee547c6af349.gif \"Trained Agent\"\n","\n","## Project background\n","The second project of Deep Reinforcement Learning ND - Udacity. The project goal is build a model that create self-learning agents (double-jointed arm) move to target locations. The goal of the agent is to maintain its position at the target location for as many time steps as possible. The threshold for a successful model is the agents must get an average score of +30 (over 100 consecutive episodes, and over all agents). This 3D environment was provided by [Udacity](https://www.udacity.com/) and is based on the [Machine learning framework](https://unity3d.com/machine-learning) provided by [Unity](https://unity.com/)\n","\n","![Trained Agent][image1]\n","\n","This project is expected to use Deep Deterministic Policy Gradient (DDPG) learning - Actor & Critic networks to train 20 virtual agents in this environment in Pytorch framework."]},{"cell_type":"markdown","metadata":{},"source":["## Continuous_Control Environment Information\n","\n","The environment will be used is [Unity MLAgents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:unityagents:\n","'Academy' started successfully!\n","Unity Academy name: Academy\n","        Number of Brains: 1\n","        Number of External Brains : 1\n","        Lesson number : 0\n","        Reset Parameters :\n","\t\tgoal_speed -> 1.0\n","\t\tgoal_size -> 5.0\n","Unity brain name: ReacherBrain\n","        Number of Visual Observations (per agent): 0\n","        Vector Observation space type: continuous\n","        Vector Observation space size (per agent): 33\n","        Number of stacked Vector Observation: 1\n","        Vector Action space type: continuous\n","        Vector Action space size (per agent): 4\n","        Vector Action descriptions: , , , \n"]}],"source":["from unityagents import UnityEnvironment\n","\n","# select this option to load version 2 (with 20 agents) of the environment\n","env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')"]},{"cell_type":"markdown","metadata":{},"source":["Agent:  \n","Each action is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector should be a number between -1 and 1.\n","\n","Environment:  \n","The observation space consists of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm.  \n","- A reward of +0.1 is provided for each step that the agent's hand is in the goal location.  \n","- State space has 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm.."]},{"cell_type":"markdown","metadata":{},"source":["## Deep Deterministic Policy Gradient Network Architecture"]},{"cell_type":"markdown","metadata":{},"source":["the agent can learn the policy either directly from the states using Policy-based methods or via the action valued function as in the case of Value-based methods. The policy-based methods tend to have high variance and low bias and use Monte-Carlo estimate whereas the value-based methods have low variance but high bias as they use TD estimates. Now, the actor-critic methods were introduced to solve the bias-variance problem by combining the two methods.\n","\n","In Actor-Critic, the actor is a neural network which updates the policy and the critic is another neural network which evaluates the policy being learned which is, in turn, used to train the actor. The actor uses the value provided by the critic to make the new policy update.  \n","\n","DDPG algorithm , a subset of Actor & Critic method, comprises two networks. The actor produces a deterministic policy instead of the usual stochastic policy and the critic evaluates the deterministic policy.  \n","\n","- Actor network composes 1 linear hidden layer taking state_size (33) as input shape, number of units by default as 256, it is come after by *relu* activation functions. The linear output layer takes the second hidden layer out_channels & action_size as params, it is followed by *tank* activation function.  \n","\n","Actor(  \n","  (fc1): Linear(in_features=33, out_features=256, bias=True)  \n","  (fc2): Linear(in_features=256, out_features=4, bias=True)  \n",")\n","\n","- Critic network composes 3 linear hidden layers taking state_size (33) as input shape, number of units by default as 256, 256 & 128 respectively. They are come after by *leaky_relu* activation functions that help to avoid *vanishing gradient* problem during training. The linear output layer takes the third hidden layer out_channels & action_size as params, it is also followed by *leaky_relu* activation function.\n","\n","Critic(  \n","  (fcs1): Linear(in_features=33, out_features=256, bias=True)  \n","  (fc2): Linear(in_features=260, out_features=256, bias=True)  \n","  (fc3): Linear(in_features=256, out_features=128, bias=True)  \n","  (fc4): Linear(in_features=128, out_features=1, bias=True)  \n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Learning algo to use\n","\n","Adam was used as an optimizer for both actor and critic networks."]},{"cell_type":"markdown","metadata":{},"source":["### Hyperparameters used\n","\n","* `BUFFER_SIZE = int(1e5)` : replay buffer size\n","* `BATCH_SIZE = 128      ` : minibatch size\n","* `GAMMA = 0.99          ` : discount factor\n","* `TAU = 1e-3            ` : for soft update of target parameters\n","* `LR_ACTOR = 1e-3       ` : learning rate of the actor\n","* `LR_CRITIC = 1e-3      ` : learning rate of the critic\n","* `WEIGHT_DECAY = 0      ` : L2 weight decay\n"]},{"cell_type":"markdown","metadata":{},"source":["## Train the agent with DDPG"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["#Case 20 agents\n","agents = Agents(state_size=state_size, action_size=action_size, n_agents=num_agents, random_seed=10)"]},{"cell_type":"markdown","metadata":{},"source":["Episode 100\tAverage Score: 0.69  \n","Episode 200\tAverage Score: 11.27  \n","Episode 274\tAverage Score: 30.05  \n","Environment solved in 174 episodes!\tAverage Score: 30.05  \n","\n","![](images/scores_epochs.jpg)  \n","The model then saved as *./saved_models/checkpoint_actor_20agents.pth* and *./saved_models/checkpoint_critic_20agents.pth* for later uses"]},{"cell_type":"markdown","metadata":{},"source":["## Testing the trained"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Saved models are reloaded in evaluation mode for testing purpose, the process is executed in CPU mode\n","agents.actor_local.load_state_dict(torch.load('./saved_models/checkpoint_actor_20agents.pth', map_location='cpu'))\n","agents.critic_local.load_state_dict(torch.load('./saved_models/checkpoint_critic_20agents.pth', map_location='cpu'))\n","agents.actor_local.eval()\n","agents.critic_local.eval()"]},{"cell_type":"markdown","metadata":{},"source":["Testing process is carried out in 20 epochs with the scores & mean as following:  \n","\n","![](images/scores_epochs_test.jpg)  "]},{"cell_type":"markdown","metadata":{},"source":["## Conclusions\n","The problem was solved using the DDPG algorithm where the average reward of +30 over at least 100 episodes was achieved in 174 episodes.  \n","The result depends significantly on the fine-tuning of the hyperparameters. Ex: if the number of time steps is too low, learning rate or the seed is too large or small, the system can fall into a local minimum where the score may start to decrease after a certain number of episodes.  \n","\n","The test was carried out in 20 epochs, the average reward of 20 agents was all above 30."]},{"cell_type":"markdown","metadata":{},"source":["## Future work to consider:\n","\n","Normally after a tranformation process, the data varies a lot and they need to be stardardized to optimum format *(Gaussian law)* of neural network (avoid outliers). Batch normalization layer is recommend to be added to the network architecture (before activation function layer) to help improve training.  \n","\n","The forward plan is working with:\n","\n","1. Proximal Policy Optimization:  \n","The idea is to implement a Policy Gradient algorithm that determines the appropriate policy with gradient methods. However, the change in the policy from one iteration to another is very slow in the neighbourhood of the previous policy in the high dimensional space.\n","\n","2. Prioritized Experience Replay:  \n","The idea behind using these technique for sampling from the replay buffer is that not all experiences are equal, some are more important than others in terms of reward, so naturally the agent should at least prioritize between the different experiences.\n","\n","3. Asynchronous Actor Critic:  \n","The idea is to have a global network and multiple agents who all interact with the environment separately and send their gradients to the global network for optimization in an asynchronous way."]},{"cell_type":"markdown","metadata":{},"source":["### References"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["https://medium.com/@jasdeepsidhu13/project-2-continuous-control-of-udacity-s-deep-reinforcement-learning-c16fef28f24e  \n","https://medium.com/@kinwo/solving-continuous-control-environment-using-deep-deterministic-policy-gradient-ddpg-agent-5e94f82f366d"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.15"}},"nbformat":4,"nbformat_minor":2}
