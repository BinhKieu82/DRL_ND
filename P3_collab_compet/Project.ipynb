{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Collaboration & Competition Project Report - DRLND."]},{"cell_type":"markdown","metadata":{},"source":["## Project background\n","Reinforcement Learning is the forth part of Machine Learing, the most complicated & continuously growing up one. Its natural is a dynamically learning by adjusting actions based in continuous feedback to maximize a reward, it opens a tremendous applications of ML in real life in helping human execute the complicated & dangerous tasks.  \n","The Third project of Deep Reinforcement Learning ND - Udacity is the simulation of how multi agents interact each other in an artifact environment to achieve a specific task . The target is build a model that enables 02 agents collaborate and compete concurently each other in the Tennis environment. The goal of each agent (player) is to keep the ball in play, thus, project's goal is score as much reward as posible through episodes. The threshold for a successful model is the agents must get an average score (2 agents) of +0.5 (over 100 consecutive episodes).  \n","Creating environment where agents can stay, interact & learn is the most difficult part. However, the environment is provided by [Udacity](https://www.udacity.com/) and is based on the [Machine learning framework](https://unity3d.com/machine-learning) provided by [Unity](https://unity.com/)  \n","A less complicated but not least part is creating agents & model. The agent has its own methods & properties that enable it interact with environment & learning. The model is the way agent explore & exploid the environment to solve project's task.  \n","\n","![](./images/tennis.gif)\n","\n","This project is expected to use (multi-agent) Deep Deterministic Policy Gradient (DDPG) learning - Actor & Critic networks to train 2 virtual agents in this environment in Pytorch framework."]},{"cell_type":"markdown","metadata":{},"source":["## Collaboration & Competition Environment Information\n","\n","The environment will be used is [Unity MLAgents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:unityagents:\n","'Academy' started successfully!\n","Unity Academy name: Academy\n","        Number of Brains: 1\n","        Number of External Brains : 1\n","        Lesson number : 0\n","        Reset Parameters :\n","\t\t\n","Unity brain name: TennisBrain\n","        Number of Visual Observations (per agent): 0\n","        Vector Observation space type: continuous\n","        Vector Observation space size (per agent): 8\n","        Number of stacked Vector Observation: 3\n","        Vector Action space type: continuous\n","        Vector Action space size (per agent): 2\n","        Vector Action descriptions: , \n"]}],"source":["env = UnityEnvironment(file_name=\"/data/Tennis_Linux_NoVis/Tennis\")"]},{"cell_type":"markdown","metadata":{},"source":["Agent:  \n","two agents control rackets to bounce a ball over a net.  \n","- Action space is two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping  \n","\n","Environment:  \n","The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket.  \n","- A reward of +0.1 is provided if an agent hits the ball over the net.  \n","- A reward of -0.01 is provided if an agent lets a ball hit the ground or hits the ball out of bounds.  "]},{"cell_type":"markdown","metadata":{},"source":["## Deep Deterministic Policy Gradient Network Architecture"]},{"cell_type":"markdown","metadata":{},"source":["the agent can learn the policy either directly from the states using Policy-based methods or via the action valued function as in the case of Value-based methods. The policy-based methods tend to have high variance and low bias and use Monte-Carlo estimate whereas the value-based methods have low variance but high bias as they use TD estimates. Now, the actor-critic methods were introduced to solve the bias-variance problem by combining the two methods.\n","\n","In Actor-Critic, the actor is a neural network which updates the policy and the critic is another neural network which evaluates the policy being learned which is, in turn, used to train the actor. The actor uses the value provided by the critic to make the new policy update.  \n","\n","DDPG algorithm , a subset of Actor & Critic method, comprises two networks. The actor produces a deterministic policy instead of the usual stochastic policy and the critic evaluates the deterministic policy.  \n","\n","- Actor network composes 1 linear hidden layer taking state_size (24) as input shape, number of units by default as 128, it is come after by *relu* activation functions. A *BatchNorm1D* layer is applied to standardize the output of hidden layer. The linear output layer takes the batchnorm output & action_size as params, it is followed by *tank* activation function.  \n","\n","Actor(  \n","   (fc1): Linear(in_features=24, out_features=128, bias=True)  \n","   (fc2): Linear(in_features=128, out_features=128, bias=True)  \n","   (fc3): Linear(in_features=128, out_features=2, bias=True)  \n","   (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True,  track_running_stats=True)  \n"," )  \n","\n","- Critic network composes 2 linear hidden layers taking state_size (24) as input shape, number of units by default as 128. They are come after by *relu* activation functions. A *BatchNorm1D* layer is applied after the input layer to standardize its output, it then is concatenated to action_size (2) to feed into 2nd hidden layer.\n","\n","Critic(  \n","   (fcs1): Linear(in_features=24, out_features=128, bias=True)  \n","   (fc2): Linear(in_features=130, out_features=128, bias=True)  \n","   (fc3): Linear(in_features=128, out_features=1, bias=True)  \n","   (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True,  track_running_stats=True)  \n"," )"]},{"cell_type":"markdown","metadata":{},"source":["## Learning algo to use\n","\n","Adam was used as an optimizer for both actor and critic networks."]},{"cell_type":"markdown","metadata":{},"source":["### Hyperparameters used\n","\n","* `BUFFER_SIZE = int(1e5)` : replay buffer size\n","* `BATCH_SIZE = 128      ` : minibatch size\n","* `GAMMA = 0.99          ` : discount factor\n","* `TAU = 1e-3            ` : for soft update of target parameters\n","* `LR_ACTOR = 1e-3       ` : learning rate of the actor\n","* `LR_CRITIC = 1e-3      ` : learning rate of the critic\n","* `WEIGHT_DECAY = 0      ` : L2 weight decay\n"]},{"cell_type":"markdown","metadata":{},"source":["## Train the agent with DDPG"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["#Case 2 agents\n","agent = Agent(state_size=state_size, action_size=action_size, num_agents=num_agents, random_seed=40)"]},{"cell_type":"markdown","metadata":{},"source":["Episode 100\tAverage Score: 0.0038  \n","Episode 200\tAverage Score: 0.0351  \n","Episode 300\tAverage Score: 0.0010  \n","Episode 400\tAverage Score: 0.0000  \n","Episode 500\tAverage Score: 0.0143  \n","Episode 600\tAverage Score: 0.0519  \n","Episode 700\tAverage Score: 0.0887  \n","Episode 800\tAverage Score: 0.1002  \n","Episode 900\tAverage Score: 0.0927  \n","Episode 1000\tAverage Score: 0.0934  \n","Episode 1100\tAverage Score: 0.1065  \n","Episode 1200\tAverage Score: 0.0867  \n","Episode 1300\tAverage Score: 0.0936  \n","Episode 1400\tAverage Score: 0.1287  \n","Episode 1496\tAverage Score: 0.5030  \n","Environment solved in 1496 episodes!\tAverage Score: 0.5030  \n","\n","![](images/scores_epochs.jpg)  \n","The model then saved as *./saved_models/checkpoint_actor.pth* and *./saved_models/checkpoint_critic.pth* for later uses"]},{"cell_type":"markdown","metadata":{},"source":["## Testing the trained"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Saved models are reloaded in evaluation mode for testing purpose, the process is executed in CPU mode\n","agent.actor_local.load_state_dict(torch.load('./saved_models/checkpoint_actor.pth', map_location='cpu'))\n","agent.critic_local.load_state_dict(torch.load('./saved_models/checkpoint_critic.pth', map_location='cpu'))\n","agent.actor_local.eval()\n","agent.critic_local.eval()"]},{"cell_type":"markdown","metadata":{},"source":["Testing process is carried out in 20 epochs with the scores & mean as following:  \n","\n","Episodes 0000-0005  Max Reward: 2.800  Moving Average: 1.900  \n","Episodes 0005-0010  Max Reward: 5.200  Moving Average: 2.020  \n","Episodes 0010-0015  Max Reward: 2.700  Moving Average: 1.920  \n","Episodes 0015-0020  Max Reward: 5.200  Moving Average: 2.130  \n","\n","![](images/scores_epochs_test.jpg)  "]},{"cell_type":"markdown","metadata":{},"source":["## Conclusions\n","The problem was solved using the MADDPG algorithm where the average reward of +0.5 over at least 100 episodes was achieved in 1496 episodes.  \n","\n","The test was carried out in 20 epochs, the average reward of 2 agents was all greater +1.5."]},{"cell_type":"markdown","metadata":{},"source":["## Future works:\n","Add some noise into the agents during the training, fine turning hyperparameters for the optimum training operation& build more complicated model to enhance the scores.  \n","\n","Try to create my own environment following the [instruction](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Getting-Started-with-Balance-Ball.md)\n"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.15"}},"nbformat":4,"nbformat_minor":2}
