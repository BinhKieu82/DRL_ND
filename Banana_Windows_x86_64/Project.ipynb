{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Navigation Project Report - Deep Reinforcement Learning ND."]},{"cell_type":"markdown","metadata":{},"source":["## Project background\n","The first project of Deep Reinforcement Learning ND - Udacity. The project goal is build a model that create an self-learning agent to navigate and collect at least 13 yellow bananas in a row of 100 episodes, in a 3D large & square world. This 3D environment was provided by [Udacity](https://www.udacity.com/) and is based on the [Machine learning framework](https://unity3d.com/machine-learning) provided by [Unity](https://unity.com/)\n","\n","![](./images/Banana.gif)\n","\n","This project is expected to use Deep Q learning to train an virtual agent in this environment in Pytorch framework."]},{"cell_type":"markdown","metadata":{},"source":["## Banana Environment Information\n","\n","The environment we will use is [Unity MLAgents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:unityagents:\n","'Academy' started successfully!\n","Unity Academy name: Academy\n","        Number of Brains: 1\n","        Number of External Brains : 1\n","        Lesson number : 0\n","        Reset Parameters :\n","\t\t\n","Unity brain name: BananaBrain\n","        Number of Visual Observations (per agent): 0\n","        Vector Observation space type: continuous\n","        Vector Observation space size (per agent): 37\n","        Number of stacked Vector Observation: 1\n","        Vector Action space type: discrete\n","        Vector Action space size (per agent): 4\n","        Vector Action descriptions: , , , \n"]}],"source":["from unityagents import UnityEnvironment\n","env = UnityEnvironment(file_name='Banana.exe')"]},{"cell_type":"markdown","metadata":{},"source":["The environment has 4 actions:\n","`0`: walk forward\n","`1`: walk backward\n","`2`: turn left\n","`3`: turn right\n","\n","State space has 37 dimensions. A reward of +1 will be awarded for collecting yellow banana and -1 for blue banana"]},{"cell_type":"markdown","metadata":{},"source":["## Deep Q-Network Architecture"]},{"cell_type":"markdown","metadata":{},"source":["A simple neural network is created as following:\n","\n","1. input_size = state_size = 37, it then fed into hidden layer 01 as input.\n","2. 2 linear hidden layers, layer 01 has 128 nodes/units output, layer 02 has 64 nodes output, both followed by relu activation functions.\n","3. A linear ouput layer with 4 outputs for 4 actions & followed by relu activation function.\n","\n","QNetwork(  \n","  (fc1): Linear(in_features=37, out_features=128, bias=True)  \n","  (fc2): Linear(in_features=128, out_features=64, bias=True)  \n","  (fc3): Linear(in_features=64, out_features=4, bias=True)  \n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Epsilon policy\n","*tanh* function from the *math package* has been used to define **epsilon policy**:\n","\n","\tepsilon = [epsilon_min+(1.0-epsilon_min)*(1-tanh(10*(i/num_episodes))) for i in range(num_episodes+10)]"]},{"cell_type":"markdown","metadata":{},"source":["## Learning algo to use\n","\n","The algo used in this project will be Q-learning algo with experience relay and fixed Q targets. The optimization of network will be done through `Adam` optimizer"]},{"cell_type":"markdown","metadata":{},"source":["### Hyperparameters used\n","\n","\n","* `BUFFER_SIZE = int(1e5)` : replay buffer size\n","* `BATCH_SIZE = 64` : minibatch size\n","* `GAMMA = 0.99` : discount factor\n","* `TAU = 1e-3` : for soft update of target parameters\n","* `LR = 5e-4` : learning rate\n","* `UPDATE_EVERY = 4` : how often to update the network\n","* `EPS_DECAY=0.995` : the reduction factor of the epsilon-greedy policy\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Train the agent with DQN"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["#instantiate an Agent\n","agent = Agent(state_size=37, action_size=4, seed=32)"]},{"cell_type":"markdown","metadata":{},"source":["Epoch:  459; Score:  20.0; Epsilon: 0.0221; Mean (100): +13.01 #  \n","Criterion reached (Mean of recent 100 runs > 13), enviroment is considered as solved!  \n","![](images/scores_epochs.jpg)  \n","The model then saved as *navigation.pth* for later uses"]},{"cell_type":"markdown","metadata":{},"source":["## Testing the trained"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Saved model is reloaded in evaluation mode for testing purpose, the process is executed in CPU mode\n","agent.qnetwork_local.load_state_dict(torch.load('navigation.pth', map_location=map_location))\n","agent.qnetwork_local.eval()"]},{"cell_type":"markdown","metadata":{},"source":["Testing process is carried out in 10 epochs with the scores & mean as following:  \n","Epoch:   10; Score:  13.0;  \n","Execution time: 29.8206 Achieved mean and standard deviation over 10 test runs: 15.10 (mea) 2.51 (std)  \n","![](images/scores_epochs_test.jpg)  "]},{"cell_type":"markdown","metadata":{},"source":["## Conclusions\n","That simple 2 hidden neural network layers model is quickly converged to the project goal, it then shows an effective perfomance on the random test."]},{"cell_type":"markdown","metadata":{},"source":["## Future work to consider:\n","\n","The forward plan is working with:\n","* Duelling DQN\n","* Double DQN\n","* Prioritized Experienced Replay"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.15"}},"nbformat":4,"nbformat_minor":2}
